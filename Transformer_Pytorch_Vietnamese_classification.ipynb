{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài toán: Phân loại các tiêu đề "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CqdCyBPJAWkD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "XgGvgSa7IqDc",
    "outputId": "8b608c73-0a99-4f10-a233-ad052eda9b48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66b5aabf8a38820e82e0b6ce</td>\n",
       "      <td>Xu hướng</td>\n",
       "      <td>100+ STT Né thính, Cap né thính hài hước, NÉT ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66b5a9838a38820e82e0b64d</td>\n",
       "      <td>Xu hướng</td>\n",
       "      <td>Top 111+ stt cuộc sống an nhiên, bình dị tự tạ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66b5cb358a38820e82e0c408</td>\n",
       "      <td>Xu hướng</td>\n",
       "      <td>Top hạt giống hoa dễ trồng, nở quanh năm cho n...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66b5c7548a38820e82e0c271</td>\n",
       "      <td>Dinh dưỡng</td>\n",
       "      <td>Chi tiết 3 cách nấu rau bò khai đơn giản mà th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66b5c7a78a38820e82e0c294</td>\n",
       "      <td>Nhà</td>\n",
       "      <td>Top 10 quạt cây hơi nước được ưa chuộng nhất h...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id       label  \\\n",
       "0  66b5aabf8a38820e82e0b6ce    Xu hướng   \n",
       "1  66b5a9838a38820e82e0b64d    Xu hướng   \n",
       "2  66b5cb358a38820e82e0c408    Xu hướng   \n",
       "3  66b5c7548a38820e82e0c271  Dinh dưỡng   \n",
       "4  66b5c7a78a38820e82e0c294         Nhà   \n",
       "\n",
       "                                               title  label_numeric  \n",
       "0  100+ STT Né thính, Cap né thính hài hước, NÉT ...              7  \n",
       "1  Top 111+ stt cuộc sống an nhiên, bình dị tự tạ...              7  \n",
       "2  Top hạt giống hoa dễ trồng, nở quanh năm cho n...              7  \n",
       "3  Chi tiết 3 cách nấu rau bò khai đơn giản mà th...              1  \n",
       "4  Top 10 quạt cây hơi nước được ưa chuộng nhất h...              4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train_set.csv')\n",
    "print(\"Training Set:\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lt6pqhFIAYSX"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.titles = dataframe['title'].str.lower().values\n",
    "        self.labels = dataframe['label_numeric'].values\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.titles[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode(title)\n",
    "        input_ids = torch.tensor(encoding, dtype=torch.long)\n",
    "        return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7LOszNGrAZRy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Collate function to pad sequences\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    input_ids = torch.stack([torch.cat([ids, torch.zeros(max_length - len(ids), dtype=torch.long)]) for ids in input_ids])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g1rriEeAAcKm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Linear layers for Q, K, V matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Output linear transformation\n",
    "        self.dense = nn.Linear(d_model, d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)   self.\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)  # (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "        )\n",
    "\n",
    "        # Layer normalization and dropout\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Split the last dimension into (num_heads, depth)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)  # (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_heads, depth)\n",
    "        # Transpose the result to shape (batch_size, num_heads, seq_len, depth)\n",
    "        return x.transpose(1, 2)  # (batch_size, seq_len, num_heads, depth) -> (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        dk = torch.tensor(k.size(-1), dtype=torch.float32)  # scalar\n",
    "        scaled_attention_logits = matmul_qk / torch.sqrt(dk)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)  # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        output = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "\n",
    "        return output, attention_weights  # (batch_size, num_heads, seq_len_q, depth_v), (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Apply linear layers and split into heads\n",
    "        q = self.split_heads(self.wq(x), batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(self.wk(x), batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "        v = self.split_heads(self.wv(x), batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Apply the custom scaled dot-product attention\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(q, k, v, mask)  # (batch_size, num_heads, seq_len_q, depth_v)\n",
    "\n",
    "        # Transpose and reshape back to (batch_size, seq_len, d_model)\n",
    "        scaled_attention = scaled_attention.transpose(1, 2).contiguous()  # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Apply the final linear layer to combine the heads\n",
    "        attn_output = self.dense(concat_attention)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = self.layernorm2(x + self.dropout(ff_output))  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        return x  # (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3QlTYsoylh3"
   },
   "source": [
    "![](https://storage.googleapis.com/mle-courses-prod/users/61b6fa1ba83a7e37c8309756/private-files/2dc6f3e0-5fb4-11ef-9b72-9db6eacc12d1-Screen_Shot_2024_08_21_at_18.54.35.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rtNUAcArIUmZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # (max_len, d_model/2)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # (max_len, d_model/2)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)  # Register as buffer so it's not a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]  # Add positional encoding, (batch_size, seq_len, d_model)\n",
    "        return x  # (batch_size, seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6KuAipzfAeXv"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, d_model, num_heads, d_ff, output_size, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_size)\n",
    "        x = self.positional_encoding(x)  # (batch_size, seq_len, d_model)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)  # (batch_size, seq_len, d_model)\n",
    "        x = x.mean(dim=1)  # (batch_size, d_model)\n",
    "        x = self.fc(self.dropout(x))  # (batch_size, output_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jdZcu7ZKBVBZ"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "train_df = pd.read_csv('train_set.csv')\n",
    "validation_df = pd.read_csv('validation_set.csv')\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "train_dataset = TextDataset(train_df, tokenizer)\n",
    "val_dataset = TextDataset(validation_df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yxaHUt79BV5c"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wfFQKb4-BX1q"
   },
   "outputs": [],
   "source": [
    "# Initialize the Transformer model\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embed_size = 256\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "output_size = len(train_df['label_numeric'].unique())\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerModel(vocab_size, embed_size, d_model, num_heads, d_ff, output_size, num_layers, dropout)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TCDm8oP1BaMl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.0404607057571411\n",
      "Validation Accuracy after Epoch 1: 66.63%\n",
      "Epoch 2/20, Loss: 1.21150541305542\n",
      "Validation Accuracy after Epoch 2: 70.12%\n",
      "Epoch 3/20, Loss: 0.8189002275466919\n",
      "Validation Accuracy after Epoch 3: 70.94%\n",
      "Epoch 4/20, Loss: 1.16700279712677\n",
      "Validation Accuracy after Epoch 4: 73.41%\n",
      "Epoch 5/20, Loss: 1.2949950695037842\n",
      "Validation Accuracy after Epoch 5: 72.38%\n",
      "Epoch 6/20, Loss: 0.8567739129066467\n",
      "Validation Accuracy after Epoch 6: 72.59%\n",
      "Epoch 7/20, Loss: 0.7920562028884888\n",
      "Validation Accuracy after Epoch 7: 75.15%\n",
      "Epoch 8/20, Loss: 0.48089343309402466\n",
      "Validation Accuracy after Epoch 8: 74.54%\n",
      "Epoch 9/20, Loss: 0.9143419861793518\n",
      "Validation Accuracy after Epoch 9: 75.36%\n",
      "Epoch 10/20, Loss: 0.8800865411758423\n",
      "Validation Accuracy after Epoch 10: 75.15%\n",
      "Epoch 11/20, Loss: 0.45223286747932434\n",
      "Validation Accuracy after Epoch 11: 75.56%\n",
      "Epoch 12/20, Loss: 1.3342427015304565\n",
      "Validation Accuracy after Epoch 12: 75.67%\n",
      "Epoch 13/20, Loss: 0.559434175491333\n",
      "Validation Accuracy after Epoch 13: 73.10%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\machine_learning\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\machine_learning\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\machine_learning\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    for input_ids, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in val_dataloader:\n",
    "            outputs = model(input_ids)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy after Epoch {epoch+1}: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
